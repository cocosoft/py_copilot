import React, { useState, useRef, useEffect } from 'react';
import './VoiceInterface.css';

const VoiceInterface = () => {
  // çŠ¶æ€ç®¡ç†
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [audioLevel, setAudioLevel] = useState(0);
  const [recognitionConfidence, setRecognitionConfidence] = useState(0);
  const [voiceSettings, setVoiceSettings] = useState({
    voiceType: 'female',
    speed: 1.0,
    pitch: 1.0,
    language: 'zh-CN',
    emotion: 'neutral'
  });
  const [audioHistory, setAudioHistory] = useState([]);
  const [recognitionStats, setRecognitionStats] = useState({
    totalRequests: 0,
    successRate: 0,
    averageConfidence: 0
  });

  // å¼•ç”¨
  const audioRef = useRef(null);
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const recognitionRef = useRef(null);

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();
      
      recognitionRef.current.continuous = true;
      recognitionRef.current.interimResults = true;
      recognitionRef.current.lang = voiceSettings.language;

      recognitionRef.current.onstart = () => {
        setIsListening(true);
        startAudioAnalysis();
      };

      recognitionRef.current.onresult = (event) => {
        let finalTranscript = '';
        let interimTranscript = '';
        let confidence = 0;

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
            confidence = event.results[i][0].confidence;
          } else {
            interimTranscript += transcript;
          }
        }

        setTranscript(finalTranscript || interimTranscript);
        setRecognitionConfidence(confidence * 100);
        
        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        updateRecognitionStats(confidence);
      };

      recognitionRef.current.onerror = (event) => {
        console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
        setIsListening(false);
        stopAudioAnalysis();
      };

      recognitionRef.current.onend = () => {
        setIsListening(false);
        stopAudioAnalysis();
      };
    }
  }, [voiceSettings.language]);

  // å¼€å§‹è¯­éŸ³è¯†åˆ«
  const startListening = () => {
    if (recognitionRef.current && !isListening) {
      try {
        recognitionRef.current.start();
      } catch (error) {
        console.error('å¼€å§‹è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      }
    }
  };

  // åœæ­¢è¯­éŸ³è¯†åˆ«
  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
    }
  };

  // å¼€å§‹éŸ³é¢‘åˆ†æï¼ˆç”¨äºæ˜¾ç¤ºéŸ³é¢‘çº§åˆ«ï¼‰
  const startAudioAnalysis = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      const source = audioContextRef.current.createMediaStreamSource(stream);
      
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      source.connect(analyserRef.current);

      const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
      
      const analyzeAudio = () => {
        if (!isListening) return;
        
        analyserRef.current.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        setAudioLevel(average);
        
        requestAnimationFrame(analyzeAudio);
      };
      
      analyzeAudio();
    } catch (error) {
      console.error('éŸ³é¢‘åˆ†æå¤±è´¥:', error);
    }
  };

  // åœæ­¢éŸ³é¢‘åˆ†æ
  const stopAudioAnalysis = () => {
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    setAudioLevel(0);
  };

  // æ–‡æœ¬è½¬è¯­éŸ³
  const speakText = async (text = transcript) => {
    if (!text.trim()) return;

    try {
      setIsSpeaking(true);
      
      // ä½¿ç”¨Web Speech API
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = voiceSettings.language;
      utterance.rate = voiceSettings.speed;
      utterance.pitch = voiceSettings.pitch;

      // è®¾ç½®è¯­éŸ³ç±»å‹
      const voices = window.speechSynthesis.getVoices();
      const selectedVoice = voices.find(voice => 
        voice.lang.includes(voiceSettings.language) && 
        voice.name.toLowerCase().includes(voiceSettings.voiceType)
      );
      
      if (selectedVoice) {
        utterance.voice = selectedVoice;
      }

      utterance.onend = () => {
        setIsSpeaking(false);
        // ä¿å­˜åˆ°å†å²è®°å½•
        addToAudioHistory(text, 'synthesis');
      };

      utterance.onerror = (error) => {
        console.error('è¯­éŸ³åˆæˆé”™è¯¯:', error);
        setIsSpeaking(false);
      };

      window.speechSynthesis.speak(utterance);
    } catch (error) {
      console.error('è¯­éŸ³åˆæˆå¤±è´¥:', error);
      setIsSpeaking(false);
    }
  };

  // åœæ­¢è¯­éŸ³åˆæˆ
  const stopSpeaking = () => {
    window.speechSynthesis.cancel();
    setIsSpeaking(false);
  };

  // æ·»åŠ åˆ°éŸ³é¢‘å†å²è®°å½•
  const addToAudioHistory = (text, type) => {
    const newEntry = {
      id: Date.now(),
      text,
      type,
      timestamp: new Date().toLocaleString(),
      settings: { ...voiceSettings }
    };
    
    setAudioHistory(prev => [newEntry, ...prev.slice(0, 49)]); // ä¿ç•™æœ€è¿‘50æ¡
  };

  // æ›´æ–°è¯†åˆ«ç»Ÿè®¡
  const updateRecognitionStats = (confidence) => {
    setRecognitionStats(prev => ({
      totalRequests: prev.totalRequests + 1,
      successRate: ((prev.successRate * prev.totalRequests + (confidence > 0.7 ? 1 : 0)) / (prev.totalRequests + 1)) * 100,
      averageConfidence: ((prev.averageConfidence * prev.totalRequests + confidence * 100) / (prev.totalRequests + 1))
    }));
  };

  // æ¸…é™¤è½¬å½•æ–‡æœ¬
  const clearTranscript = () => {
    setTranscript('');
    setRecognitionConfidence(0);
  };

  // ä¸‹è½½éŸ³é¢‘
  const downloadAudio = async (text) => {
    try {
      // è¿™é‡Œåº”è¯¥è°ƒç”¨åç«¯APIç”ŸæˆéŸ³é¢‘æ–‡ä»¶
      // æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿä¸‹è½½
      const blob = new Blob([text], { type: 'text/plain' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `voice_${Date.now()}.txt`;
      a.click();
      URL.revokeObjectURL(url);
    } catch (error) {
      console.error('ä¸‹è½½éŸ³é¢‘å¤±è´¥:', error);
    }
  };

  // è¯­éŸ³è®¾ç½®å˜æ›´
  const handleVoiceSettingChange = (setting, value) => {
    setVoiceSettings(prev => ({
      ...prev,
      [setting]: value
    }));
  };

  return (
    <div className="voice-interface">
      {/* è¯­éŸ³è¯†åˆ«æ§åˆ¶åŒºåŸŸ */}
      <div className="recognition-section">
        <div className="recognition-controls">
          <button 
            className={`listen-btn ${isListening ? 'active' : ''}`}
            onClick={isListening ? stopListening : startListening}
            disabled={isSpeaking}
          >
            {isListening ? (
              <>
                <div className="pulse-animation"></div>
                ğŸ¤ åœæ­¢å½•éŸ³
              </>
            ) : (
              'ğŸ¤ å¼€å§‹å½•éŸ³'
            )}
          </button>
          
          <div className="audio-level">
            <div 
              className="level-bar" 
              style={{ width: `${audioLevel}%` }}
            ></div>
          </div>
          
          <div className="confidence-indicator">
            <span>ç½®ä¿¡åº¦: {recognitionConfidence.toFixed(1)}%</span>
          </div>
        </div>
        
        <div className="transcript-area">
          <textarea
            value={transcript}
            onChange={(e) => setTranscript(e.target.value)}
            placeholder="è¯­éŸ³è¯†åˆ«ç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."
            className="transcript-input"
            rows="4"
          />
          <div className="transcript-actions">
            <button onClick={clearTranscript} className="clear-btn">
              ğŸ—‘ï¸ æ¸…é™¤
            </button>
            <button onClick={() => speakText()} className="speak-btn" disabled={isSpeaking || !transcript.trim()}>
              {isSpeaking ? 'ğŸ”Š æ’­æ”¾ä¸­...' : 'ğŸ”Š æœ—è¯»'}
            </button>
          </div>
        </div>
      </div>

      {/* è¯­éŸ³åˆæˆæ§åˆ¶åŒºåŸŸ */}
      <div className="synthesis-section">
        <div className="voice-settings">
          <h3>è¯­éŸ³è®¾ç½®</h3>
          
          <div className="setting-group">
            <label>è¯­éŸ³ç±»å‹</label>
            <select 
              value={voiceSettings.voiceType}
              onChange={(e) => handleVoiceSettingChange('voiceType', e.target.value)}
            >
              <option value="female">å¥³å£°</option>
              <option value="male">ç”·å£°</option>
              <option value="child">ç«¥å£°</option>
            </select>
          </div>
          
          <div className="setting-group">
            <label>è¯­é€Ÿ: {voiceSettings.speed.toFixed(1)}x</label>
            <input 
              type="range" 
              min="0.5" 
              max="2" 
              step="0.1" 
              value={voiceSettings.speed}
              onChange={(e) => handleVoiceSettingChange('speed', parseFloat(e.target.value))}
            />
          </div>
          
          <div className="setting-group">
            <label>è¯­è°ƒ: {voiceSettings.pitch.toFixed(1)}x</label>
            <input 
              type="range" 
              min="0.5" 
              max="2" 
              step="0.1" 
              value={voiceSettings.pitch}
              onChange={(e) => handleVoiceSettingChange('pitch', parseFloat(e.target.value))}
            />
          </div>
          
          <div className="setting-group">
            <label>è¯­è¨€</label>
            <select 
              value={voiceSettings.language}
              onChange={(e) => handleVoiceSettingChange('language', e.target.value)}
            >
              <option value="zh-CN">ä¸­æ–‡</option>
              <option value="en-US">English</option>
              <option value="ja-JP">æ—¥æœ¬èª</option>
            </select>
          </div>
          
          <div className="setting-group">
            <label>æƒ…æ„Ÿ</label>
            <select 
              value={voiceSettings.emotion}
              onChange={(e) => handleVoiceSettingChange('emotion', e.target.value)}
            >
              <option value="neutral">ä¸­æ€§</option>
              <option value="happy">é«˜å…´</option>
              <option value="sad">æ‚²ä¼¤</option>
              <option value="excited">å…´å¥‹</option>
            </select>
          </div>
        </div>
        
        <div className="quick-actions">
          <h3>å¿«æ·æ“ä½œ</h3>
          <div className="action-buttons">
            <button onClick={() => speakText('ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ')} disabled={isSpeaking}>
              ğŸ‘‹ é—®å€™è¯­
            </button>
            <button onClick={() => speakText('è°¢è°¢æ‚¨çš„ä½¿ç”¨ï¼Œå†è§ï¼')} disabled={isSpeaking}>
              ğŸ‘‹ å‘Šåˆ«è¯­
            </button>
            <button onClick={stopSpeaking} disabled={!isSpeaking}>
              â¹ï¸ åœæ­¢æ’­æ”¾
            </button>
          </div>
        </div>
      </div>

      {/* å†å²è®°å½•åŒºåŸŸ */}
      <div className="history-section">
        <h3>å†å²è®°å½•</h3>
        <div className="history-list">
          {audioHistory.length === 0 ? (
            <p className="no-history">æš‚æ— å†å²è®°å½•</p>
          ) : (
            audioHistory.map(item => (
              <div key={item.id} className="history-item">
                <div className="item-header">
                  <span className="item-type">{item.type === 'synthesis' ? 'ğŸ”Š' : 'ğŸ¤'}</span>
                  <span className="item-time">{item.timestamp}</span>
                </div>
                <p className="item-text">{item.text}</p>
                <div className="item-actions">
                  <button onClick={() => speakText(item.text)}>æ’­æ”¾</button>
                  <button onClick={() => downloadAudio(item.text)}>ä¸‹è½½</button>
                </div>
              </div>
            ))
          )}
        </div>
      </div>

      {/* ç»Ÿè®¡ä¿¡æ¯åŒºåŸŸ */}
      <div className="stats-section">
        <h3>è¯†åˆ«ç»Ÿè®¡</h3>
        <div className="stats-grid">
          <div className="stat-item">
            <span className="stat-value">{recognitionStats.totalRequests}</span>
            <span className="stat-label">æ€»è¯·æ±‚æ•°</span>
          </div>
          <div className="stat-item">
            <span className="stat-value">{recognitionStats.successRate.toFixed(1)}%</span>
            <span className="stat-label">æˆåŠŸç‡</span>
          </div>
          <div className="stat-item">
            <span className="stat-value">{recognitionStats.averageConfidence.toFixed(1)}%</span>
            <span className="stat-label">å¹³å‡ç½®ä¿¡åº¦</span>
          </div>
        </div>
      </div>

      {/* éŸ³é¢‘æ’­æ”¾å™¨ */}
      <audio ref={audioRef} className="audio-player" />
    </div>
  );
};

export default VoiceInterface;